#+TITLE: [What]RAIDs
#+DATE: <2020-07-02 Thu> 
#+TAGS: CS
#+LAYOUT: post
#+CATEGORIES: book,ostep
#+NAME: <book_ostep_RAIDs.org>
#+OPTIONS: ^:nil
#+OPTIONS: ^:{}

看看大师是如何解释磁盘阵列的。

这章对性能计算部分还没有理解透彻，还需要花时间来回顾一下。
#+BEGIN_EXPORT html
<!--more-->
#+END_EXPORT
RAID(Redundant Array of Inexpensive Disks) 是由多个磁盘组合而成的存储系统，使得存储更为快速、容量更大、数据更安全。
* RAID 的接口
对于文件系统来讲，RAID 被抽象化为了一个可读写的硬盘。
当文件系统发出 I/O 请求时，RAID 的管理系统会来识别应该对哪些硬盘进行操作。
* RAID Level 0: striping
Level 0 模式下，RAID 将所有硬盘组合成一个大的硬盘，其容量很大，性能也较强。

每个硬盘的 block 是 4 K，那么假设有 4 块硬盘，它们在逻辑上 block 的排列可以如下：
| Disk 0 | Disk 1 | Disk 2 | Disk 3 |
|--------+--------+--------+--------|
|      0 |      1 |      2 |      3 |
|      4 |      5 |      6 |      7 |
|      8 |      9 |     10 |     11 |
|     12 |     13 |     14 |     15 |

这样子当 I/O 请求读写 16 KB 连续数据的话，RAID 可以并行的操作 4 块硬盘读写。

当然还有其他的排列方式：
| Disk 0 | Disk 1 | Disk 2 | Disk 3 |
|--------+--------+--------+--------|
|      0 |      2 |      4 |      6 |
|      1 |      3 |      5 |      7 |
|      8 |     10 |     12 |     14 |
|      9 |     11 |     13 |     15 |

每个硬盘每两个 block 组合成一个 chunk，大小为 8 KB，抽象出来 RAID 操作一行的大小就是 32 KB.
** chunk size 的大小
chunk size 的大小会影响 RAID 系统整体的性能，假设访问一个同样大小的文件：
- 当 chunk size 比较小时，RAID 会将操作拆分为多个并行处理，这提高了并发度。但是由于要同时操作多块硬盘，每次都需要每个硬盘定位成功以后才能进行操作，这又增加了定位时间。
  + 定位时间是多块硬盘中定位最长的时间
- 当 chunk size 比较大时，RAID 可能只需要操作其中一块硬盘就可以满足，这降低了并发度。但由于仅仅需要定位一块硬盘，所以其定位时间就降低了。
** RAID-0 的缺陷
由于数据是依次存放在各个硬盘上的，一旦其中任何一个硬盘损坏，那么数据便会损坏了。
- 比如假设一个硬盘一部分 block 损坏了，而这部分是有关文件系统 metadata ，那么有可能整个文件系统的数据都无法很好的浏览了。
** 评估 RAID 的性能
RAID 的性能评估一般从下面两个指标：
1. 单次请求延迟
2. 稳态吞吐率

吞吐量的测试又分为顺序读写和随机读写两种。
- 对于磁盘来讲顺序读写模式下，没有大量的磁头移动和等待磁盘转动到目的位置的时间，所以其吞吐量很大。
- 而随机读写模式下磁头移动和等待磁盘转动消耗了太多时间，所以其吞吐量很小，远远小于顺序读写模式。
  
对于 RAID-0 来讲：
- 其单次请求延迟等同于单个硬盘的请求延迟
- 稳态吞吐量等同于多个硬盘的吞吐量之和
* RAID Level 1:Mirroring
Level 1 就是用一个块硬盘对另外一块硬盘进行镜像拷贝，这样能大概率避免磁盘数据错误。

| Disk 0 | Disk 1 | Disk 2 | Disk 3 |
|--------+--------+--------+--------|
|      0 |      0 |      1 |      1 |
|      2 |      2 |      3 |      3 |
|      4 |      4 |      5 |      5 |
|      6 |      6 |      7 |      7 |

如上表所示：
- 当需要读一个 block 的时候，RAID 可以原则读原磁盘还是其镜像磁盘。
  + 其读取延迟就是选择磁盘中的最大延迟
- 当需要写一个 block 的时候，RAID 需要并行的操作原磁盘和其镜像磁盘。
  + 其写入延迟就是选中磁盘中的最大延迟
  + 在写入时，需要保证写入同步的原子性
  
但是与 RAID Level 0 相比，因为每个磁盘都需要另一个磁盘做镜像，所以对用户来讲，整体容量就只有所有磁盘容量的一半。

于此对应的，其顺序读写的吞吐量也只有 Level 0 一半，但随机读与 Level 0 没什么差别（因为都是要查找一块来读取），
而随机写的性能比不上 Level 0 （因为需要原子的完成两块硬盘的写入同步）。
* RAID Level 4: Saving Space With Parity
由于 Level 1 消耗了一半的磁盘空间，这实在是太浪费了。

Level 4 的方式就是为数据增加对应的校验，来达到节省空间的目的。
- 当然，生成校验是会消耗 CPU 资源的。
  
| Disk 0 | Disk 1 | Disk 2 | Disk 3 | Disk 4 |
|--------+--------+--------+--------+--------|
|      0 |      1 |      2 |      3 | P0     |
|      4 |      5 |      6 |      7 | P1     |
|      8 |      9 |     10 |     11 | P2     |
|     12 |     13 |     14 |     15 | P3     |

如上表所示，P0 便是对 0，1，2，3 的校验 block。

校验是通过异或改行的数据每一位而得到的异或结果，如下表所示：
| Block 0 | Block 1 | Block 2 | Block 3 | Parity |
|---------+---------+---------+---------+--------|
|      00 |      10 |      11 |      10 |     11 |
|      10 |      01 |      00 |      01 |     10 |

通过从左到右进行异或，可以发现：每一行（包括校验位）对应的位的 1 的个数总是偶数个。

假设 Block 2 的第 0 位丢失了，那么可以通过同样的方式对其它位进行异或（包括校验位），便可以还原出原来的位。
- 当然，如果多余 1 位丢失了，那数据就无法还原了。所以从硬盘的角度上来讲也可以说是这种方式可以容忍 1 块硬盘的数据丢失

这种方式的存储容量就是： (N - 1) * B
- N 是硬盘总共的数量
- B 是一个硬盘所有的 block 数量
  
可以看出来其容量比 Level 1 还是大多了。

从性能上来讲，Level 4 的顺序读吞吐率为：(N - 1) * S MB/s
- N 是硬盘总共的数量
- S 是一块硬盘顺序读的速度
  
随机读的吞吐率为：(N - 1) * R MB/s
- R 是一块硬盘随机读的速度
  
而顺序写的步骤是：
- 用户将多个 block 写给 RAID
- RAID 在接收数据的同时对同一行数据进行异或运算
  + 由于异或运算的速度快于 IO 传输的速度，所以整体性能上并不会有明显的损失
- 最终 RAID 将接收到的数据并行的写入磁盘

从上面可以得出，Level 4 的顺序写吞吐率为：(N - 1) * S MB/s

而随机写入的情况要复杂一点，当用户仅写一个 block 时，与该 block 同一行的校验 block 也需要更新。有下面两种方法：
- 将同一行的其它 block 也读出来，然后与新写入的 block 进行异或运算，写入校验 block 中
- 只读取将要写入的 block 和 校验 block，使用公式进行计算： P_new = (Block_old XOR Block_new) XOR P_old
  
无论是上面方法中的哪一种，每一次的写都伴随着一次读，所以随机写的速率只有单块随机读的一半。
* RAID Level 5: Rotating Parity
Level 4 在进行随机写时，可能会遇到校验块顺序写入的性能瓶颈问题。

| Disk 0 | Disk 1 | Disk 2 | Disk 3 | Disk 4 |
|--------+--------+--------+--------+--------|
|      0 |      1 |      2 |      3 | P0     |
|      4 |      5 |      6 |      7 | P1     |
|      8 |      9 |     10 |     11 | P2     |
|     12 |     13 |     14 |     15 | P3     |

如上表所示，假设用户随机写的是块 5 和块 14，本来这两个块处于两个不同的磁盘是可以完成并行的读写。
但是由于需要更新校验块 P1 和 P3，而它们是在同一块磁盘，这就导致了由于 P1 和 P3 的串行读写化，
也使得块 5 和 块 14 的读写也得串行化。

Level 5 就将校验块分散在了各个磁盘：
| Disk 0 | Disk 1 | Disk 2 | Disk 3 | Disk 4 |
|--------+--------+--------+--------+--------|
|      0 |      1 |      2 |      3 |     P0 |
|      5 |      6 |      7 |     P1 |      4 |
|     10 |     11 |     P2 |      8 |      9 |
|     15 |     P3 |     12 |     13 |     14 |
|     P4 |     16 |     17 |     18 |     19 |

按这种方式存储的话，假设用户随机写的还是块 5 和块 14，此时块 5 及校验块 P1 和块 14及校验块 P3 都分处在不同的硬盘。
这样就可以完成并行的读写操作了。

[[./pic/raid.jpg]]

上图便是各种 RAID 模式下的比较。
