* [What] zynq --> petalinux2016.2 -> linux4.4

** 配置
zynq linux 的配置流程与 u-boot 极为相似, 传统的方式也是使用命令 =make menuconfig=, 在 petalinux 下则使用 =petalinux-config -c kernel ,  petalinux-config -c rootfs=.
由界面生成的配置文件位于 =project/subsystem/linux/configs/kernel/config,project/subsystem/linux/configs/genericl/config,project/subsystem/linux/configs/rootfs/config=.
有关设备树的配置文件位于 =project/subsystem/linux/configs/device-tree/=
顶层的 Makfile 有两个主要任务是产生 vmlinux 文件和内核模块.为了实现此功能,顶层 Makfile 递归的进入到内核的各个子目录中,分别调用位于这些子目录中的 Makfile.
*** petalinux 自动配置
运行命令 =petalinux-config= 在界面中选择 =Auto Config Settings=, 可以选择哪些组件需要自动更新,如果没有选中则需要用户主动更新.
- 设备树自动配置文件
  - skeleton.dtsi , zynq-7000.dtsi, pcw.dtsi, pl.dtsi. system-conf.dtsi
- kernel 自动配置文件
  - config 
- rootfs 自动配置文件
  - config
- u-boot 自动配置文件
- config, platform-auto.h
一般来说都需要打开自动配置,如果觉得不合理的地方, 可以在 top 文件中使用 =undef= 来屏蔽
*** 文件结构
通过 petalinux 生成的 image.ub 文件包含了如下内容:
- 设备树
- 内核
- 根文件系统, 包括必备的文件结构, 用户代码, 用户库以及用户模块.
** 分析
*** 启动分析
**** 内存分布分析
内存分析需要分析基本的链接脚本,与 arm 相关的脚本有 =arch/arm/boot/compressed/vmlinux.lds.S= 以及 =arch/arm/kernel/vmlinux.lds.S=.
在linux启动输出的过程中,可以看到如下信息:
[[linux_ld.jpg]]
此信息在 =mm/page_alloc.c中的 mem_init_print_info= 中打印.
关于 vmlinux.lds.S 中的段会按照 =include/asm-generic/vmlinux.lds.h= 进行组织.
- __init_begin - __init_end:
只有初始化才会使用到的段放在这个区间,一旦初始化完成,那么这个区间里的数据或代码在后面就不会被使用,内核会把这部分内存释放出来.
要完整的查看函数的地址,变量的地址,链接段的地址信息,位于文件 =build/linux/kernel/<linux>/System.map=.
通过查看 =arch/arm/kernel/vmlinux.lds.S= 可以知道 =ENTRY(stext)= 说明运行的第一条指令从 stext 符号处启动, 此符号位于 =arch/arm/kernel/head.S=
***** 起始地址
通过查看 System.map 可以知道, stext 的地址为 0xc008000.
通过查看 =arch/arm/kernel/vmlinux.lds.S= 知道链接的虚拟起始地址为 *. =PAGE_OFFSET + TEXT_OFFSET*.
- PAGE_OFFSET
定义位于文件 =arch/arm/include/asm/memory.h= ,定义为 *#define PAGE_OFFSET  UL(CONFIG_PAGE_OFFSET)*,
而在 =subsystem/linx/configs/kernel/config= 中可以知道, CONFIG_PAGE_OFFSET 的值为 *0XC0000000*.
- TEXT_OFFSET
代表内核在RAM中的起始位置相对于RAM起始地址的偏移.此值的定义位于 =arch/arm/Makefile=.
*textofs-y := 0x00008000*
*TEXT_OFFSET := $(textofs-y)*

*值为 0x8000* 的原因:
kernel 镜像的前16K需要预留出来给初始化页表使用.

**** 启动之前的准备
在kernel启动之前的环境,都是由 bootloader来准备实现的,一般会有如下几个步骤:
***** 1. 将kernel镜像加载到RAM的对应位置
*需要注意的是*: 加载位置是有要求的, 一般是加载到物理 RAM 偏移 0X8000(32K)的位置,这32K中16K作为启动参数,16K作为临时页表.而kernel会从加载的位置上开始解压.
***** 2. 硬件环境初始化
****** 关闭MMU
MMU关闭的情况下,CPU寻址的地址都是物理地址. *映射表需要由kernel来创建* .所以在kernel主动创建映射表之前,必须保证MMU处于关闭状态.
****** 关闭数据缓存
数据缓存一定要关闭,否则可能kernel刚启动的过程中,读数据的时候,从Cache 中读取, *而Cache中的数据由可能是在kernel加载之前的数据*, 从而会导致异常发生.
****** 寄存器 r0 = 0
****** 寄存器 r1 = machine ID
****** 寄存器 r2 = atags or dtb pointer
***** 3. PC指针跳转到 kernel 的入口代码处运行
**** 启动
从 *内存分布分析* 中可以知道:
- kernel 的入口虚拟地址为 *0xc008000*
- kernel 的入口代码为 =arch/arm/kernel/head.S= 的 *ENTRY(stext)* 处
***** 1. 准备工作
在打开MMU之前,运行地址与链接脚本的虚拟地址不一致,所以使用的是 *位置无关汇编代码*.
主要流程为:
- 进入 SVC 模式, 关闭所有中断
- 获取 CPU ID, 提取 proc info
- 验证 tags或 dtb
- 创建临时内核页表和页表项
- 配置寄存器 r13
- 使能MMU
- 跳转到 start_kernel
***** 2. start_kernel
此阶段主要由 C 代码来完成,并且此阶段的代码 *都是在虚拟内存上运行的* ,start_kernel 文件位于 =init/main.c=.
它主要完成第一阶段没有初始化的与硬件平台相关的初始化工作. 在完成一系列与内核相关的初始化后,调用第一个用户进程(init进程)并等待用户进程的执行,这样整个linux内核便启动完毕.
该函数所做的工作包括:
1. 进行体系结构相关的第一个初始化工作
2. 初始化系统核心进程调度器和时钟中断处理机制
3. 初始化串口控制台
4. 创建和初始化系统 cache ,为各种内存调用机制提供缓存.
5. 初始化内存管理.
6. 初始化系统的进程间通信机制
7. 调用函数 =rest_init()= , 启动第一个进程. init 进程首先进行一系列的硬件初始化, 然后通过命令行传递过来的参数挂载根文件系统.

当所有的初始化工作结束后, cpu_idle()函数会被调用来使系统处于闲置状态(idle),并等待用户进程的执行.

**** 文件启动
***** 文件系统挂载
默认情况下, petalinux 将文件系统集成在 image.ub文件中, 这样在启动内核后, 内核直接将其拷贝到 RAM中运行, *但这期间所做的修改是不会写回到image.ub中了*, 说白了就是一个虚拟文件系统了.
为了能够存储, 需要做如下步骤:
1. 将存储文件系统的设备或分区格式化为 ext4 文件格式
2. 使用 =petalinux-config -> Image Packaging Configuration -> Root filesystem type= , 选择对应设备类型, 此时 =system-conf.dtsi=设备树的 =bootargs= 会更新.
3. 编译系统,并导出根文件压缩包 =petalinux-package --image -c rootfs --format initramfs=
4. 解压根文件包 =sudo pax -rvf rootfs.cpio=
5. 将解压后的结果拷贝进对应设备分区
***** 流程概述
1. 当载入文件系统以后,首先读入 =/boot= 目录下的内核文件.
2. 运行第一个程序 =/sbin/init=,用户初始化系统环境, 进程PID是1,其他的进程都是它的子进程.
3. 根据运行级别来确定启动哪些守护进程.设置运行级别的文件位于 =/etc/inittab=,而相对于运行级别而对应的运行程序文件位于 =/etc/rc5.d/= ("rc" 是 "run command" 的缩写, "d" 是 "directory" 的缩写, zynq 默认运行级别是5)
此目录下的文件格式都是 "S + XX + 程序名", "S" 代表 "Start", 数字代表运行的先后顺序, 数字越小越早处理, 数字相同时,按照字符顺序启动.
4. 加载开机启动程序
开机启动程序脚本统一的放在 =/etc/init.d/= 下
***** 用户登录
****** 命令行登录:
init 进程调用 gettty 程序,让用户输入用户名和密码. 输入完成后,再调用 login 程序,然后核对密码. 如果密码正确,就从文件 =/etc/passwd= 读取用户指定的 shell, 然后启动shell
****** ssh登录
系统调用 sshd 程序, 取代 getty 和 login 然后启动 shell
****** 图形界面登录
init 进程调用显示管理器, 如果用户密码正确就读取 =/etc/gdm3/Xsession= 启动用户会话.
***** 配置shell环境
先读取 =/etc/profile= 中的用户配置, 然后依次寻找 =~/.bash_profile, ~/.bash_login, ~/.profile= 配置文件(只要找到其中一个, 后面的就不会寻找了)
*** 实现linux的快速启动
zynq的启动流程为: ROMBOOT -->  fsbl --> uboot -> kernel
后面3个阶段都是可定制的, 那么要想快速启动, 需要把握原则:
1. 仅仅初始化启动所必要的硬件, 关于硬件的深度检测, 放在 bootloader 的用户选项来执行
2. 软件过程中尽量不要有死等, 尽量运用非阻塞方式编程
3. 为了让用户 *感觉启动很快*, 需要 *尽量早的显示开机画面,在开机画面停留的几秒时间再来配置硬件及软件环境*.

**** bootloader 的快速启动
** 驱动
*** DMA engine
参考于: [[www.wowotech.net][蜗窝科技]]
**** 基本概念
***** DMA channels
一个 DMA controller 可以同时进行的DMA传输的个数是有限的,这称为 DMA channels.
*注意*: 这里的 "channel"仅仅是一个逻辑概念.
因为鉴于总线访问的冲突,以及内存一致性的考量,从物理的角度看,不大可能会同时进行两个及以上的DMA传输.因而DMA channel 不太可能是物理上独立的通道.
很多时候,DMA channels 是 DMA controller 为了方便而抽象出来的概念, 让 consumer 以为独占了一个 channel, 实际上所有的channel的DMA传输请求都会在DMA controller
中进行仲裁,进而串行传输.
因此,软件也可以基于 controller 提供的channel(物理),自行抽象更多的逻辑channel, 软件会管理这些逻辑channel 上的传输请求.实际上很多平台都这样做了, 在DMA engine framework 中,
不会区分这两种channel(本质上没有区别).
***** DMA request lines
DMA传输的设备和DMA控制器之间,会有几条物理的连接线,称为 DMA request(DRQ), 用于通知DMA 控制器可以开始传输了.每个数据收发的节点,称作 endpoint 和 DMA controller 之间,就有一条 DMA request line.

DMA channel 是 provider , DMA request line 是 Consumer, 在一个系统中 DMA request line 的数量通常比 DMA channel 的数量多, 因为并不是每个 request line 在每一个时刻都需要数据传输.
***** 传输参数
****** transfer size
传输的数据大小
****** transfer width
传输数据宽度
****** burst size
DMA 控制器内部可缓存的数据量大小
***** scatter-gather
将不连续地址的数据传输到一个连续的缓存中.

 

**** 使用介绍
***** 概念
站在DMA 的视角来看, 无论传输的是什么方向,都是slave 于 slave之间的数据传输.
*但是在 memory 到 memory 这种情况下*, Linux 为了方便基于DMA的 memcpy,memset等操作,在dma engine 上又封装了一层
更为简洁的 API,这种 API 就是 Async TX API(以async_开头, 比如 async_memcpy, async_memset, async_xor等).
[[.dma_engineAPI,jpg]]
除此之外的3种情况(MEM2DEV,DEV2MEM,DEV2DEV)被称为Slave-DMA传输.
*注意*: 在Slave-DMA中的 "slave",值的是参与DMA传输的设备, 而对应的 "master"指的是 DMA controller 自身.
***** consumer使用步骤
****** 申请channel
DMA channel 在kernel 中由 "struct dma_chan" 数据结构表示, 由 provider 提供操作
使用函数: =struct dma_chan *dma_request_chan(struct device *dev, const char *name)=
该接口会返回绑定在设备上名称为name的dma channel .
当不使用 channel 时,使用函数释放: =void dma_release_channel(struct dma_chan *chan);=
*注意*:
在petalinux2016.2中申请channel对应的函数为 =dma_request_slave_channel= 
****** 配置 channel参数
使用配置函数 =int dmaengine_slave_config(struct dma_chan *chan, struct dma_slave_config *config)= 进行配置
****** 获取描述符
DMA传输属于异步传输,在启动传输之前,slave driver 需要将此次传输的一些信息提交给dma engine, dma engine 确定后,返回描述符 dma_async_tx_decriptor.
此后, slave driver 就可以以该描述符为单位,控制并跟踪此次传输.
有3个 API 可以获取传输描述符:
1. struct dma_async_tx_descriptor *dmaengine_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl, unsigned int sg_len, enum dma_data_direction direction, unsigned long flags);
用于在sg列表和总线设备之间进行DMA传输.
2. struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len, size_t period_len, enum dma_data_direction direction);
用于音频等场景中,进行一定长度的DMA 传输.
3. struct dma_async_tx_descriptor *dmaengine_prep_interleaved_dma(struct dma_chan *chan, struct dma_interleaved_template *xt, unsigned long flags);
****** 提交并启动
1. 提交描述符 =dma_cookie_t dmaengine_submit(struct dma_async_tx_descriptor *desc)=
返回一个唯一识别该描述符的 cookie,用于后续的跟踪和监控
2. 启动传输 =void dma_async_issue_pending(struct dma_chan *chan)=
****** 等待传输结束
传输请求被提交之后,client driver 可以通过回调函数获取传输完成的消息,当然也可以通过 =dma_async_is_tx_complete= 等API,测试传输是否完成.\
****** 停止传输
1. dmaengine_pause
2. dmaengine_resume
3. dmaengine_terminate_all



***** provider 编写
****** 整体描述
DMA 控制器驱动需要做的事有:
1. 抽象并控制DMA控制器
2. 管理 DMA channel (可以是物理channel,也可以是虚拟channel),并向client driver 提供友好,易用的接口.
3. 以 DMA channel 为操作对象,响应 client driver的传输请求,并控制DMA controller执行传输.
主要思路如下:
[[./dma_engine.jpg]]
1. 使用 =struct dma_device= 抽象 DMA controller, controller driver 只要填充该结构中必要的字段,就可以完成驱动开发.
2. 使用 =struct dma_chan= 抽象物理的 DMA channel, 物理的channel 和 controller 所能提供的通道数一一对应.
3. 基于物理的 DMA channel ,使用 =struct virt_dma_cha= 抽象出虚拟的 dma channel.多个虚拟channel 可以共享一个物理channel,并且在这个物理channel上进行分时传输.
4. 基于这些数据结构,提供一些便于controller driver 开发的API.
****** 数据结构
1. struct dma_device 主要元素
- channels 
链表头, 用于保存该 controller 支持的所有 dma_channel(struct dma_chan). 在初始化的时候, dma controller driver 首先要调用INIT_LIST_HEAD初始化它,
然后调用 =list_add_tail= 将所有的channel 添加到该链表头中.
- cap_mask
一个bitmap, 用于指示该dma controller 所具备的能力(可以进行什么样的DMA传输)(具体参考 =enum dma_transaction_type= 定义).
该bitmap的定义,需要和后面device_prep_dma_xxx形式的回调函数对应.
- src_addr_widths
一个bitmap,表示该 controller 支持哪些宽度的src类型.
- dst_addr_widths
一个bitmap,表示该 controller 支持哪些宽度的dst类型.
- directions
一个bitmap,表示该controller支持哪些传输方向.
- max_burst
支持的突发传输大小
- descriptor_reuse
表示该 controller 的传输描述符是否可以重复使用
- device_alloc_chan_resources / device_free_chan_resources
client driver 申请/释放 dma channel的时候, dmaengine会调用对应函数,以准备相应的资源.
- device_prep_dma_xxx 
与client driver 的 dmaengine_perp_xxx API对应.
- device_config
与client driver 的 dmaengine_slave_config 相对应.
- device_pause/device_resume/ device_terminate_all.
与 client driver 的暂停,停止函数相对应.
- device_issue_pending
与 client driver 的 dma_async_issus_pending 相对应
2. struct dma_chan 主要元素
用于抽象 dma channel .
- device 
指向 dma controller
- cookie
client driver 以该 channel 为操作对象获取传输描述符时, dma controller driver 返回给 client 的最后一个 cookie
- completed_cookie
在这个channel 上最后一次完成的传输的 cookie,dma controller driver 可以在传输完成时调用辅助函数 =dma_cookie_complete= 设置它的值
- device_node
链表的node, 用于将该channel 添加到 dma_device的 channel 列表中.
3. struct virt_dma_cha 
用于抽象一个虚拟的 dma channel ,多个虚拟channel可以共用一个物理channel,并由软件调度多个传输请求, 将多个虚拟channel 的传输串行地在物理channel上完成.
- chan
一个 struct dma_chan 类型, 用于和client driver 打交道.(屏蔽物理channel 和 虚拟channel的差异).
- task
一个 tasklet, 用于等待该虚拟 channel 上传输的完成.
- desc_allocated, desc_submitted, desc_issued, desc_completed,
用于保存不同状态的虚拟 channel 描述符.





****** 使用函数
1. 注册和注销
=int dma_async_device_register(struct dma_device *device);=
=void dma_async_device_unregister(struct dma_device *device);=
2. cookie有关的辅助接口
=static inline void dma_cookie_init(struct dma_chan *chan)=
=static inline dma_cookie_assign(struct dma_async_tx_descriptor *tx)=
=static inline void dma_cookie_complete(struct dma_async_tx_descriptor *tx)=
=static inline enum dma_status dma_cookie_status(struct dma_chan *chan, dma_cookie_t cookie, struct dma_tx_state *state)=
3, 依赖处理接口
=void dma_run_dependencies(struct dma_async_tx_descriptor *tx);=
client可以同时提交多个具有依赖关系的dma传输,因此当某个传输结束的时候, dma controller driver 需要检查是否由依赖该传输的传输,
4. 设备树接口
=struct dma_chan *of_dma_simple_xlate(struct of_phandle_args *dma_spec, struct of_dma *ofdma);=
=struct dma_chan *of_dma_xlate_by_chan_id(struct of_phandle_args *dma_spec, struct of_dma *ofdma)=
用于将client device node 中有关的dma的字段解析出来, 并获取对应的 dma channel.

****** 编写 dma controller driver 的步骤
1. 定义 struct dma_device 变量,并初始化
2. 根据 controller 支持的 channel 个数,为每个channel 定义一个 struct dma_chan 变量,进行必要的初始化后,将每个channel都添加道 struct dma_device变量的channels链表中.
3, 根据硬件特性,实现 struct dma_device 变量中必要的回调函数
4. 调用 dma_async_device_register 将 struct dma_device 变量注册到 kernel 中
5. 当 client driver 申请 dma channel 时,dmaengine core 会调用 dma controller driver 的 device_alloc_chan_resources函数,controller driver 需要在这个接口中将该channel 资源准备号.
6. 当client driver配置某个 dma channel 时, dmaengine core会调用 dma controller driver 的device_config函数,controller driver 需要在这个函数将对应配置应用好
7. client driver 开始一个传输之前,会把传输的信息通过 dmaengine_prep_slave_xxx接口交给controller driver,controller driver 需要在对应的device_prep_dma_xx回调中准备好, 并返回给client driver 一个描述符
8. 然后client driver 会调用 dmaengine_submit 将该传输提交给 controller driver,此时 dmaengine 会调用controller driver 为每个传输描述符所提供的 tx_submit回调函数,controller driver 需要在这个函数中将描述符挂到该channel对应的传输队列中
9. client driver 开始传输时, 会调用 dma_async_issue_pending, controller driver 需要在对应的回调函数(device_issue_pending)中,依次将队列上所有的传输请求提交给硬件.
